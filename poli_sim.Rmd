---
title: "Poli et al. (2020) sims"
author: "Mike"
date: "12/2/2020"
output: html_document
---

```{r setup}
library(tidyverse)
library(permute)
```

Goal is to understand the relationship between information-theoretic measures over sequences. 

Computational reproducibility exercise with Poli, Serino, Mars, Hunnius (2020). Infants tailor their attention to maximize learning. *Science Advances*. 

Sequences are sequences of locations in a grid with four quadrants. 

# Sequences

> We created the sequences in MATLAB. First, 16 sequences were sampled pseudo-randomly, with the probabilities specified above as only constraint. Then, the sequences were concatenated. To check that the target location could be predicted only by relying on cue- target conditional probabilities, we fed the result of the sampling into a machine learning random forest classifier. If the classifier was able to reliably predict the target location with no information about the cue-target conditional probabilities (e.g., it successfully predicted the target location at trial N only based on target location at trial N-1), then the entire process was repeated and new sequences were sampled. 


```{r}
sample_seq <- rep(1:4, 4)[shuffle(1:15)]
```

# Learning model

> The model is presented with a set of events x. An event is, for example, the target appearing in the upper left corner of the screen. The events followed each other until the sequence ended (or the in- fant looked away). The last event of a sequence, which also coincides with the length of the sequence, is named j, and the sequence can thus be denoted by Xj = {x1,...,xj}. The first goal of the model is to estimate the probability with which a certain event x will occur. Given that the target can appear in one of four possible locations k, the distribution of probabilities can be parameterized by the random vector p = [p1, ..., pk], where pk is the probability of the target ap- pearing in the kth location. In our specific case, the target locations are four, and thus, p = [p1, p2, p3, p4]. The ideal learner treats p1 : 4 as parameters that must be estimated trial by trial given Xj. In other words, given the past events up until the current trial, the ideal learner will estimate the probabilities with which the target will appear in any of the four possible target locations.
At the very beginning of each sequence, the ideal learner expects the target to appear in one of the four target locations with equal probability. This is expressed here as a prior Dirichlet distribution
P(p∣ ) = Dir(p; k) (1)
where all elements of  are equal to one,  = [1,1,1,1]. In this case, the parameter  determines prior expectations. If there is an imbal- ance between the values of  (e. g. ,  = [100,1,1,1]), this means that the model is biased into thinking that the target is more likely to ap- pear in the one location (p1 in the example) rather than the others.

Dirichlet-multinomial is the same thing as the beta binomial. 

Play with beta-binomial:

```{r}
x = seq(0,1,.01)
# shape parameters are the number of counts
plot(x, dbeta(x, 200,400))

```


```{r}
prior <- c(1,1,1,1)
```


> Conversely, when the numbers are equal, the ideal learner has no biases toward any location. Moreover, high numbers indicate that the model has strong expectations, while low numbers indicate that the model will quickly change its expectations when presented with new evidence. Thus, specifying  = [1,1,1,1], we are defining a weak uniform prior distribution. In other words, the model has no bias toward any location but is ready to change these expectations if pre- sented with contradicting evidence.
At every trial, the prior distribution is updated given the obser- vation of the new event x from the set Xj. 
>  The posterior distribution of such update is given by
P(p∣Xj,)=Dir(pj;njk+k) (2)
j
where nk refers to the number of outcomes of type k observed up
until the trial j. As a practical example, imagine that, at trial 1, the model observes a target in the location 1 (i.e., [1, 0, 0, 0]). The values of  will be updated with the evidence accumulated, thus moving from [1, 1, 1, 1] to [2, 1, 1, 1]. This implies that now it is slightly more likely to see the target in location 1 than in any of the other loca- tions. Specifically, the probability of the target appearing in any lo- cation can be computed from the posterior distribution P(p∣Xj, ) in the following fashion
p(x =k∣X ,)=─j−1+K (3)
>In words, how likely the target is to appear in a certain corner is given by the total number of times it appeared in that corner, plus one (the value of ), divided by the total number of observations, plus 4 (the sum of the values of ). This updating rule implies that as evidence accumulates, new evidence will weigh less. Given that our sequences are stationary (i.e., the most likely location does not change within the same sequence), this assumption is justified for the current task.
At every trial j, the posterior Dirichlet distribution of trial j − 1 becomes the new prior distribution. The new prior is updated using (2) and the probabilities estimates are computed using (3). When infants look away and a new sequence is played, the prior is set back to (1). This means that we assume that when infants start looking to a new sequence, they consider it as independent of the previous ones. Previous research in adults demonstrated the suitability of this assumption (24).

```{r}
n <- 15
model <- tibble(trials = 0:n, 
                bin1 = 0, 
                bin2 = 0, 
                bin3 = 0, 
                bin4 = 0)

model[1,2:5] <- as.list(prior)

for (i in 1:length(sample_seq)) {
  model[model$trials == i, 2:5] <- model[model$trials == i-1, 2:5]
  model[model$trials == i, sample_seq[i]+1] <-  
    model[model$trials == i-1, sample_seq[i]+1] + 1
}
```



# Measures

* Surprisal
* Predictability
* Learning progress

## Surprisal 

The ideal learner model uses information theory (24) to compute the surprise of each event, the predictability of the sequence at each trial, and the learning progress at each trial. Surprise is quantified in terms of Shannon Information, I
j
I(xj = k)=−log2p(xj = k∣Xj−1,) (4) = k) is the probability that an event x (i.e., the appearance
where p(x
of the target) will occur in a given location k (e.g., the upper left cor- ner). This probability depends on the prior  and on the evidence accumulated on the previous trials, Xj−1. By taking the negative logarithm of a probability, events that are highly probable will have low levels of surprise, while low-probability events will have a high level of surprise.

## Predictability

Predictability is quantified it terms of negative entropy, −H jKjjjj
(5)


Note that, different from surprise, here, predictability is estimated considering also the event j, and not just up to j − 1. This formula was applied when relating predictability to infants’ looking away and looking time, as they have the information relative to trial j when they decide whether to look away and when they look at the target of trial j. However, saccadic latencies do not depend on Xj but rather on Xj−1, as when planning a saccade toward the target of trial j, the target has not appeared yet. Hence, a formula slightly different from (5) was used when relating predictability to saccadic latencies, in which Xj was replaced by Xj−1.

## Learning progress

Last, the learning progress is quantified in terms of Kullback- Leibler Divergence (or information gain), DKL
 n j−1 + 1 j j−1 k
the estimate of the parameters p1 : k that was performed on the pre- vious trial j − 1. Learning progress has been defined as the reduction in the error of an agent’s prediction (15). DKL is the divergence be- tween a weighted average of prediction error at trial j and a weighted average of prediction error at trial j − 1, and hence, it is a suitable way to model learning progress in this task.